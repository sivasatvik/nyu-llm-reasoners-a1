\documentclass{article}
\usepackage[UTF8]{ctex}
\usepackage[margin=0.7in]{geometry}
\usepackage{enumitem}
\usepackage{listings}


\begin{document}

\begin{enumerate}[start=2,label=\arabic*.]
    \item Byte-Pair Encoding (BPE) Tokenizer
        \begin{enumerate}[label=\arabic*.]
            \item Understanding Unicode
                \begin{enumerate}[label=\alph*.]
                    \item \texttt{chr(0)} returns '\verb|b'\x00|''
                    \item \texttt{\_\_repr\_\_()} shows an escaped, unambiguous form whereas its printed representation outputs the character itself which is actual NULL character(invisible).
                    \item When we add it to text, it simply adds an invisible character at that position which may not be visually noticeable.
                \end{enumerate}
            \item Unicode Encodings
                \begin{enumerate}[label=\alph*.]
                    \item UTF-8 is preferred over UTF-16 or UTF-32 due to its superior compression, universal applicability without out of vocabulary errors, and native compability with byte-stream data.
                    \item If we give the input ç‰›, it breaks and gives an error "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe7 in position 0: unexpected end of data". This is due to the fact that the decoding is done byte by byte and when the encoded character is multi-byte, it fails to decode it properly.
                    \item  \verb|b'\xC3\x28|' is invalid in UTF-8 because 0xC3 indicates the start of a 2-byte character, but 0x28 is not a valid continuation byte.
                \end{enumerate}
            \item BPE Training on TinyStories
                Training on the given tinystories dataset produces the following output:
                \begin{lstlisting}
(venv) sivasatvik@10-17-88-31 nyu-llm-reasoners-a1 % 
time python student/train_bpe_tinystories.py 
--input ./data/TinyStoriesV2-GPT4-train.txt
Elapsed (s): 61.72
RSS (MB): 158.3
Longest token length (bytes): 15
Longest token (latin-1):  accomplishment
python student/train_bpe_tinystories.py --input
319.31s user
14.65s system
539% cpu
1:01.86 total
                \end{lstlisting}
                It seems that the longest token is `accomplishment' with a length of 15 bytes. The memory usage is around 158.3 MB and the training took approximately 61.72 seconds. It makes sense that longer tokens are formed from frequently occurring sequences of characters in the dataset, and `accomplishment' might be a common word in the TinyStories dataset.
        \end{enumerate}
    \item Transformer Language Model Architecture
        \begin{enumerate}[label=\arabic*.]
            \item Subsection 1
                \begin{enumerate}[label=\alph*.]
                    \item a
                    \item b
                \end{enumerate}
            \item Subsection 2
                \begin{enumerate}[label=\alph*.]
                    \item a
                    \item b
                    \item c
                \end{enumerate}
        \end{enumerate}
\end{enumerate}

\end{document}